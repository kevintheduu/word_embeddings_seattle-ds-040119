{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep NLP - Word Embeddings\n",
    "\n",
    "Think back to NLP as we've understood it so far.\n",
    "\n",
    "If we've had some luck with NLP modeling, likely with a NaiveBayes algorithm, we were able to illustrate some correlations between words and some other feature of interest.\n",
    "\n",
    "But to whatever extent that our models were able to make connections and pick up on correlations, they did this *without any understanding of the **meaning** of the words in question*.\n",
    "\n",
    "Let's think for a minute about words and objective meanings!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can make sense of meaning for computational purposes by thinking about meaning in terms of similarity, i.e. thinking about meaning *holistically*.\n",
    "\n",
    "Q. Is there any precedent for this way of thinking about meaning? <br/>\n",
    "A. [Yes](https://plato.stanford.edu/entries/meaning-holism/#ArgForMeaHol)\n",
    "\n",
    "So what will this look like for us?\n",
    "\n",
    "*Remember cosine similarity?*\n",
    "\n",
    "$\\rightarrow$We'll have much the same idea here: Associate each word with values along particular dimensions in a multi-dimensional space. If we had a dimension for *softness*, for example, then pillows and marshmallows would score higher on it than rocks and bricks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "import numpy as np\n",
    "import string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reading in the data\n",
    "\n",
    "import json\n",
    "\n",
    "with open('JEOPARDY_QUESTIONS1.json') as f:\n",
    "    data = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "list"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let's check the datatype of our data\n",
    "type(data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "216930"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# And the length\n",
    "len(data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'category': 'HISTORY',\n",
       " 'air_date': '2004-12-31',\n",
       " 'question': \"'For the last 8 years of his life, Galileo was under house arrest for espousing this man's theory'\",\n",
       " 'value': '$200',\n",
       " 'answer': 'Copernicus',\n",
       " 'round': 'Jeopardy!',\n",
       " 'show_number': '4680'}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let's look at the first element in our list\n",
    "\n",
    "data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "98"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# How many words do we have in our first question?\n",
    "\n",
    "len(data[0]['question'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "18"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let's try that again!\n",
    "\n",
    "len(data[0]['question'].split(' '))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3169994"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let's count the total number of\n",
    "# clue words we have.\n",
    "length = 0\n",
    "\n",
    "for clue in data:\n",
    "    length += len(clue['question'].split(' '))\n",
    "    \n",
    "length\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'!\"#$%&\\'()*+,-./:;<=>?@[\\\\]^_`{|}~'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "string.punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Word2Vec requires that our text have the form of a list\n",
    "# of 'sentences', where each sentence is itself a list of\n",
    "# words. How can we put our _Jeopardy!_ clues in that shape?\n",
    "\n",
    "\n",
    "import string\n",
    "text = []\n",
    "for clue in data:\n",
    "    sentence = clue['question'].translate(str.maketrans('', '', string.punctuation )).split(' ')\n",
    "    new_sent = []\n",
    "    for word in sentence:\n",
    "        new_sent.append(word.lower())\n",
    "    text.append(new_sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['for',\n",
       " 'the',\n",
       " 'last',\n",
       " '8',\n",
       " 'years',\n",
       " 'of',\n",
       " 'his',\n",
       " 'life',\n",
       " 'galileo',\n",
       " 'was',\n",
       " 'under',\n",
       " 'house',\n",
       " 'arrest',\n",
       " 'for',\n",
       " 'espousing',\n",
       " 'this',\n",
       " 'mans',\n",
       " 'theory']"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let's check the new structure of our first clue\n",
    "\n",
    "text[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Constructing the model is simply a matter of\n",
    "# instantiating a Word2Vec object.\n",
    "model = gensim.models.Word2Vec(text,sg=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(11334978, 15849970)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# To train, call 'train()'!\n",
    "\n",
    "model.train(text, total_examples=model.corpus_count,epochs=model.epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3169994"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Checking word  count\n",
    "\n",
    "model.corpus_total_words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## model.wv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 3.72395486e-01, -2.98815846e-01,  2.28254348e-01, -5.64172745e-01,\n",
       "        1.99366678e-02, -1.00599742e+00, -2.47957736e-01,  3.96134257e-01,\n",
       "       -2.01570511e-01,  5.21437109e-01,  2.14793414e-01,  1.15296625e-01,\n",
       "       -6.21627927e-01, -1.17822021e-01, -4.52635109e-01, -7.22625703e-02,\n",
       "       -1.24124236e-01,  6.76122010e-01, -7.30100870e-01,  2.12803543e-01,\n",
       "        2.69140780e-01,  2.79574513e-01,  1.01608738e-01,  1.78896829e-01,\n",
       "        1.32047549e-01, -7.51025826e-02,  6.40465319e-01, -2.03592941e-01,\n",
       "       -4.14902061e-01,  4.33540404e-01, -6.79211505e-03,  3.30879629e-01,\n",
       "       -4.35412049e-01, -3.37042838e-01,  5.84698170e-02, -3.53708901e-02,\n",
       "       -5.92287183e-02,  2.08368436e-01,  2.57964462e-01,  7.22370088e-01,\n",
       "        1.64817289e-01, -1.86656073e-01,  1.82483539e-01, -6.30457699e-02,\n",
       "       -2.19198972e-01, -3.89395691e-02, -3.31805050e-01, -2.13700399e-01,\n",
       "       -1.39999673e-01,  3.31333578e-01, -1.41252980e-01,  3.92775536e-01,\n",
       "        6.74875230e-02, -2.61344999e-01,  2.47770876e-01, -5.36838889e-01,\n",
       "        5.38676500e-01,  2.64995486e-01, -3.48736852e-01, -2.33751863e-01,\n",
       "        1.72051817e-01, -4.72307876e-02, -3.60801935e-01, -3.20895046e-01,\n",
       "        1.50175244e-01, -3.63201827e-01,  4.07342583e-01,  1.26083255e-01,\n",
       "        2.48923942e-01, -2.75435150e-01, -6.39436841e-02,  2.39096284e-01,\n",
       "        1.72762930e-01,  1.91643283e-01,  2.33747914e-01,  2.39430875e-01,\n",
       "        7.29807361e-04, -3.40670168e-01,  1.19974516e-01,  1.99993074e-01,\n",
       "       -2.48635769e-01,  1.08885676e-01, -1.76961243e-01,  1.38647676e-01,\n",
       "       -2.86440581e-01, -2.75674343e-01, -4.65559095e-01, -3.28561962e-02,\n",
       "        2.04929605e-01,  5.70998676e-02, -2.23904252e-01,  4.17884171e-01,\n",
       "       -5.05493283e-01, -3.04893404e-01,  1.36353709e-02, -4.51114684e-01,\n",
       "       -1.27110630e-01,  3.30174953e-01,  2.06233442e-01, -2.21284091e-01],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# The '.wv' attribute stores the word vectors\n",
    "model.wv['child']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 287,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The vectors are keyed by the words\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### model.wv methods\n",
    "#### 'most_similar()' and 'similarity()'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('artwork', 0.737768292427063),\n",
       " ('ceramic', 0.7291909456253052),\n",
       " ('drip', 0.7034561634063721),\n",
       " ('chippendale', 0.7032985687255859),\n",
       " ('pottery', 0.6973737478256226),\n",
       " ('decorative', 0.6966370344161987),\n",
       " ('accessory', 0.6901211738586426),\n",
       " ('locking', 0.6854533553123474),\n",
       " ('frenchnamed', 0.6854013204574585),\n",
       " ('fastener', 0.6806753873825073)]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.wv.most_similar('furniture')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.66396564"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.wv.similarity('furniture', 'jewelry')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('dog', 0.7088381052017212),\n",
       " ('feline', 0.6822689175605774),\n",
       " ('shorthaired', 0.6811498403549194),\n",
       " ('cheetah', 0.6760207414627075),\n",
       " ('hound', 0.6699329614639282),\n",
       " ('possum', 0.6644060015678406),\n",
       " ('rabbit', 0.6562787890434265),\n",
       " ('pup', 0.6532145142555237),\n",
       " ('raccoon', 0.6494170427322388),\n",
       " ('breed', 0.6475555300712585)]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# What's most similar to 'cat'?\n",
    "model.wv.most_similar('cat')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('queen', 0.6575067043304443),\n",
       " ('throne', 0.6181319952011108),\n",
       " ('aquitaine', 0.6009867787361145),\n",
       " ('monarch', 0.5821282267570496),\n",
       " ('princess', 0.5732308626174927),\n",
       " ('margrethe', 0.5705289840698242),\n",
       " ('prince', 0.5614256262779236),\n",
       " ('granddaughter', 0.5610052347183228),\n",
       " ('noor', 0.5577331781387329),\n",
       " ('elizabeth', 0.5556656718254089)]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let's try the familiar example: King - Man + Woman = Queen\n",
    "\n",
    "model.wv.most_similar(positive=['king','woman'],\n",
    "                        negative=['man'], topn=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('sophocles', 0.7457695007324219),\n",
       " ('shakespeares', 0.7407026290893555),\n",
       " ('euripides', 0.7270536422729492),\n",
       " ('shakespearean', 0.6873356699943542),\n",
       " ('moliere', 0.6746113300323486),\n",
       " ('falstaff', 0.6724187135696411),\n",
       " ('ibsen', 0.6645510196685791),\n",
       " ('hamlet', 0.6622061729431152),\n",
       " ('rur', 0.652551531791687),\n",
       " ('shaws', 0.6513330340385437)]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Shakespeare\n",
    "model.wv.most_similar(['shakespeare'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('kinnear', 0.8371871709823608),\n",
       " ('connors', 0.8036473989486694),\n",
       " ('shaun', 0.7964942455291748),\n",
       " ('bebe', 0.7916409969329834),\n",
       " ('dennehy', 0.7871823906898499),\n",
       " ('prinze', 0.7821978330612183),\n",
       " ('dwayne', 0.7804005146026611),\n",
       " ('elfman', 0.7784302830696106),\n",
       " ('conner', 0.7763600945472717),\n",
       " ('arkin', 0.7739968299865723)]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Greg\n",
    "model.wv.most_similar(['greg'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('dc', 0.8015254735946655),\n",
       " ('arlington', 0.6565327644348145),\n",
       " ('washingtons', 0.6517009735107422),\n",
       " ('dca', 0.6256974935531616),\n",
       " ('abilene', 0.6205648183822632),\n",
       " ('dcs', 0.6142348051071167),\n",
       " ('richland', 0.6126922369003296),\n",
       " ('pataki', 0.6084930300712585),\n",
       " ('delaware', 0.5991126298904419),\n",
       " ('p3', 0.5936689376831055)]"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Washington\n",
    "model.wv.most_similar(['washington'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 'doesnt_match()'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'breakfast'"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.wv.doesnt_match(['breakfast','lunch','dinner','viagra'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'bush'"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.wv.doesnt_match(['tree','flower','bush','plant','toothbrush'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'toothbrush'"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.wv.doesnt_match(['tree','flower','plant','toothbrush'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 'closer_than()'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 295,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Which words are closer to 'king' than 'queen' is?\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 'distance()'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'int' object is not iterable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-55-edf1fab06999>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;31m# normalize our vectors.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0mvector\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwv\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m     \u001b[0mnorm_vecs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/gensim/models/keyedvectors.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, entities)\u001b[0m\n\u001b[1;32m    352\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_vector\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mentities\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    353\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 354\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mvstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_vector\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mentity\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mentity\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mentities\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    355\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    356\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__contains__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mentity\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: 'int' object is not iterable"
     ]
    }
   ],
   "source": [
    "# For this it will make more sense to\n",
    "# normalize our vectors.\n",
    "\n",
    "for vector in model.wv:\n",
    "    norm_vecs.map()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 199,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.wv.distance('king', 'king')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.4097934365272522"
      ]
     },
     "execution_count": 206,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.wv.distance('joy', 'happiness')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 'evaluate_word_analogies()'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check out [this text file](https://raw.githubusercontent.com/nicholas-leonard/word2vec/master/questions-words.txt)!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/krex/anaconda3/lib/python3.7/site-packages/smart_open/smart_open_lib.py:398: UserWarning: This function is deprecated, use smart_open.open instead. See the migration notes for details: https://github.com/RaRe-Technologies/smart_open/blob/master/README.rst#migrating-to-the-new-open-function\n",
      "  'See the migration notes for details: %s' % _MIGRATION_NOTES_URL\n"
     ]
    }
   ],
   "source": [
    "relatives = model.wv.evaluate_word_analogies('https://raw.githubusercontent.com/nicholas-leonard/word2vec/master/questions-words.txt')[1][4]\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "149"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(relatives['correct'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "271"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(relatives['incorrect'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('BOY', 'GIRL', 'BROTHER', 'SISTER'),\n",
       " ('BOY', 'GIRL', 'GRANDSON', 'GRANDDAUGHTER'),\n",
       " ('BOY', 'GIRL', 'HE', 'SHE'),\n",
       " ('BOY', 'GIRL', 'HIS', 'HER'),\n",
       " ('BOY', 'GIRL', 'KING', 'QUEEN')]"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "relatives['correct'][:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('BOY', 'GIRL', 'BROTHERS', 'SISTERS'),\n",
       " ('BOY', 'GIRL', 'DAD', 'MOM'),\n",
       " ('BOY', 'GIRL', 'FATHER', 'MOTHER'),\n",
       " ('BOY', 'GIRL', 'GRANDFATHER', 'GRANDMOTHER'),\n",
       " ('BOY', 'GIRL', 'GRANDPA', 'GRANDMA')]"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "relatives['incorrect'][:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
